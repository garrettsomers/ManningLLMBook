{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/garrettsomers/ManningLLMBook/blob/chapter3/notebooks/Chapter_3-Text_Classification_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHC2u6uR6iWU"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install openprompt\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tlj4ZSPjAXGe"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FFCfAYMGlHg"
   },
   "outputs": [],
   "source": [
    "## DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVoX8wRW4dWk"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('glue', 'sst2')\n",
    "seed = 0\n",
    "\n",
    "# keep only 1000 of the training data to speed up preprocessing/tokenization\n",
    "# we're doing few shot so don't need a lot\n",
    "dataset['train'] = dataset['train'].shuffle(seed=seed).select(range(1000))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q49b4VKByBp3"
   },
   "outputs": [],
   "source": [
    "# separate the positives and negatives in training to ensure balanced samples\n",
    "# this could matter a lot in few-shot\n",
    "dataset['pos_train'] = dataset['train'].filter(lambda x: x['label']==1)\n",
    "dataset['neg_train'] = dataset['train'].filter(lambda x: x['label']==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8pnkofx0VgZ",
    "outputId": "3674c1f5-ce74-4e67-9cf9-89eeec36455e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 32, 64, 128, 256]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will try different samples evenly split between classes\n",
    "shot_increments = 5\n",
    "sample_sizes = [2**i for i in range(4, 4 + shot_increments)]\n",
    "sample_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFKjNUDyGnTm"
   },
   "outputs": [],
   "source": [
    "## START OF PROMPTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afHBOgUy09w8"
   },
   "outputs": [],
   "source": [
    "from openprompt.data_utils import InputExample\n",
    "\n",
    "# create a dataset of opemprompt InputExamples from the training data\n",
    "prompt_dataset = {}\n",
    "for split in ['pos_train', 'neg_train', 'validation', 'test']:\n",
    "    prompt_dataset[split] = []\n",
    "    for data in dataset[split]:\n",
    "        input_example = InputExample(text_a = data['sentence'], label=int(data['label']), guid=data['idx'])\n",
    "        prompt_dataset[split].append(input_example)\n",
    "print(prompt_dataset['pos_train'][0])\n",
    "print(prompt_dataset['neg_train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAiFkS1q4VAw"
   },
   "outputs": [],
   "source": [
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts import ManualTemplate, ManualVerbalizer\n",
    "from openprompt import PromptDataLoader\n",
    "\n",
    "# load the BERT model\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-base-cased\")\n",
    "\n",
    "# create the prompt template\n",
    "template_text = '{\"placeholder\": \"text_a\"} it is {\"mask\"} .'\n",
    "template = ManualTemplate(tokenizer=tokenizer, text=template_text)\n",
    "\n",
    "# create a wrapped tokenizer\n",
    "wrapped_tokenizer = WrapperClass(max_seq_length=128, decoder_max_length=3, tokenizer=tokenizer, truncate_method=\"head\")\n",
    "\n",
    "# define your verbalizer with desired vocabulatary mapping to pos and neg\n",
    "verbalizer = ManualVerbalizer(tokenizer, num_classes=2,\n",
    "                              label_words=[['terrible'], ['great']])\n",
    "\n",
    "# generate a testing dataloader\n",
    "val_dataloader = PromptDataLoader(prompt_dataset['validation'], template, tokenizer=tokenizer,\n",
    "                                  tokenizer_wrapper_class=WrapperClass, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zUrV00f7eZ0"
   },
   "outputs": [],
   "source": [
    "# define the accuracy metric\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "    with torch.no_grad():\n",
    "        for step, inputs in enumerate(val_dataloader):\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            logits = model(inputs)\n",
    "            labels = inputs['label']\n",
    "            alllabels.extend(labels.cpu().tolist())\n",
    "            allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wh5bPmH8ghd"
   },
   "outputs": [],
   "source": [
    "from openprompt import PromptForClassification\n",
    "\n",
    "# zero-shot testing: run the evalation set against the prompt model before\n",
    "# any finetuning.\n",
    "\n",
    "prompt_model = PromptForClassification(plm=copy.deepcopy(plm), template=template,\n",
    "                                       verbalizer=verbalizer)\n",
    "prompt_model = prompt_model.cuda()\n",
    "prompt_scores = [evaluate(prompt_model, val_dataloader)]\n",
    "prompt_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RI5LWM775Pq-"
   },
   "outputs": [],
   "source": [
    "# now generate a learning curve. loop over different values of k (total samples)\n",
    "# and calculate accuracy for each\n",
    "\n",
    "from transformers import AdamW\n",
    "\n",
    "for k in sample_sizes:\n",
    "  # they are already shuffled, we can simply select the first k examples each time\n",
    "  train_sample = prompt_dataset['pos_train'][:k] + prompt_dataset['neg_train'][:k]\n",
    "  train_dataloader = PromptDataLoader(train_sample, template, tokenizer=tokenizer,\n",
    "                                    tokenizer_wrapper_class=WrapperClass, shuffle=True,\n",
    "                                    batch_size=4, seed=seed)\n",
    "  \n",
    "  prompt_model = PromptForClassification(plm=copy.deepcopy(plm), template=template,\n",
    "                                         verbalizer=verbalizer, freeze_plm=False)\n",
    "  prompt_model = prompt_model.cuda()\n",
    "    \n",
    "  loss_func = torch.nn.CrossEntropyLoss()\n",
    "  no_decay = ['bias', 'LayerNorm.weight']\n",
    "  optimizer_grouped_parameters = [\n",
    "      {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "      {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "  ]\n",
    "\n",
    "  optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "\n",
    "  for epoch in range(5):\n",
    "    tot_loss = 0\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "  accuracy = evaluate(prompt_model, val_dataloader)\n",
    "  prompt_scores.append(accuracy)\n",
    "  print('Test set accuracy:', accuracy)\n",
    "  torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UBk8VinGge2"
   },
   "outputs": [],
   "source": [
    "## START OF FINETUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SyAJ6rSFnX4h"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer \n",
    "\n",
    "# instantiate a BERT tokenizer and tokenizer the dataset\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence\"], truncation=True)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# rename columns and convert tokenized dataset to pytorch format\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fpYGSp5WSXT"
   },
   "outputs": [],
   "source": [
    "# define the accuracy metrid\n",
    "\n",
    "def compute_acc(eval_preds):\n",
    "  preds = np.argmax(eval_preds.predictions, axis=-1)\n",
    "  labels = eval_preds.label_ids\n",
    "  acc = sum([int(i==j) for i,j in zip(preds, labels)])/len(labels)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJLE2D2Jmtan"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import datasets\n",
    "\n",
    "# \"zero-shot\" i.e. the head has random weights and no training is done \n",
    "training_args = TrainingArguments(\"trainer\")\n",
    "finetune_model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2)\n",
    "if use_cuda:\n",
    "  finetune_model = finetune_model.cuda()\n",
    "\n",
    "trainer = Trainer(\n",
    "    finetune_model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    # data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "  )\n",
    "\n",
    "# skip training of the \"Trainer\"\n",
    "preds = trainer.predict(tokenized_datasets['validation'])\n",
    "acc = compute_acc(preds)\n",
    "finetune_scores = [acc]\n",
    "print(finetune_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gAFDEOlsHg3J"
   },
   "outputs": [],
   "source": [
    "# now generate a learning curve. loop over different values of k (total samples)\n",
    "# and calculate accuracy for each\n",
    "\n",
    "for k in sample_sizes:\n",
    "  train_sample = datasets.concatenate_datasets([tokenized_datasets['pos_train'].select(range(k)),\n",
    "                                                tokenized_datasets['neg_train'].select(range(k))])\n",
    "  training_args = TrainingArguments(\"trainer\")\n",
    "  model = copy.deepcopy(finetune_model)\n",
    "  if use_cuda:\n",
    "    model = model.cuda()\n",
    "  \n",
    "  trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=train_sample,\n",
    "    # data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    "  )\n",
    "\n",
    "  trainer.train()\n",
    "  preds = trainer.predict(tokenized_datasets['validation'])\n",
    "  acc = compute_acc(preds)\n",
    "  finetune_scores.append(acc)\n",
    "  print(finetune_scores)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdJ-WM4HG__F"
   },
   "outputs": [],
   "source": [
    "## LEARNING CURVE COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJAJ2epYg2is"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VvaMyJUnwjx4"
   },
   "outputs": [],
   "source": [
    "# plot the prompt-based training and pre-train/finetuning learning curves\n",
    "# against each other\n",
    "\n",
    "fig1 = plt.figure()\n",
    "ax11 = fig1.add_subplot(111)\n",
    "\n",
    "x = [0] + sample_sizes\n",
    "ax11.plot(range(len(x)), finetune_scores, color='r', label='finetuned model')\n",
    "ax11.scatter(range(len(x)), finetune_scores, color='r')\n",
    "ax11.plot(range(len(x)), prompt_scores, color='b', label='prompt model')\n",
    "ax11.scatter(range(len(x)), prompt_scores, color='b')\n",
    "ax11.set_xticks(range(len(x)))\n",
    "ax11.set_xticklabels(x)\n",
    "ax11.set_xlabel('Number of training examples (per class)')\n",
    "ax11.set_ylabel('Test Set Accuracy')\n",
    "ax11.legend()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
