{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/garrettsomers/ManningLLMBook/blob/chapter8/Chapter_8_Retreival_Augmented_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "AoA9ej5vIrUE",
        "outputId": "84b29495-cbaf-43aa-b185-be1fba6b57d3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.9/622.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llama-index-embeddings-adapter 0.1.0 requires llama-index-core==0.10.0, but you have llama-index-core 0.10.19 which is incompatible.\n",
            "llama-index-finetuning 0.1.0 requires llama-index-core==0.10.0, but you have llama-index-core 0.10.19 which is incompatible.\n",
            "llama-index-llms-gradient 0.1.0 requires llama-index-core==0.10.0, but you have llama-index-core 0.10.19 which is incompatible.\n",
            "llama-index-llms-openai 0.1.0 requires llama-index-core==0.10.0, but you have llama-index-core 0.10.19 which is incompatible.\n",
            "llama-index-postprocessor-cohere-rerank 0.1.0 requires llama-index-core==0.10.0, but you have llama-index-core 0.10.19 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llama-index-embeddings-huggingface 0.1.1 requires llama-index-core<0.11.0,>=0.10.1, but you have llama-index-core 0.10.0 which is incompatible.\n",
            "llama-index-embeddings-openai 0.1.1 requires llama-index-core<0.11.0,>=0.10.1, but you have llama-index-core 0.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llama-index-embeddings-huggingface 0.1.1 requires llama-index-core<0.11.0,>=0.10.1, but you have llama-index-core 0.10.0 which is incompatible.\n",
            "llama-index-embeddings-openai 0.1.1 requires llama-index-core<0.11.0,>=0.10.1, but you have llama-index-core 0.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install llama-index-finetuning==0.1.0 -q -U\n",
        "%pip install llama-index-embeddings-openai==0.1.1 -q -U\n",
        "%pip install llama-index-embeddings-huggingface==0.1.1 -q -U\n",
        "%pip install llama-index-llms-openai==0.1.0 -q -U\n",
        "%pip install openai==1.12.0 -q -U\n",
        "%pip install sentence_transformers -q -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-_hWDju9Cqd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "4a10e92a-4da1-44ae-f4a7-c589cf776aea"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Archive:  /content/drive/MyDrive/llm-book-data/openai_tos.zip\n",
            "   creating: ./openai_tos/\n",
            "  inflating: ./openai_tos/09_sharing-and-publication-policy.txt  \n",
            "  inflating: ./openai_tos/02_privacy-policy.txt  \n",
            "  inflating: ./openai_tos/07_usage-policies.txt  \n",
            "  inflating: ./openai_tos/05_plugins-and-actions-terms.txt  \n",
            "  inflating: ./openai_tos/08_enterprise-privacy-at-openai.txt  \n",
            "  inflating: ./openai_tos/06_business-terms.txt  \n",
            "  inflating: ./openai_tos/01_terms-of-use.txt  \n",
            "  inflating: ./openai_tos/03_service-terms.txt  \n",
            "  inflating: ./openai_tos/04_data-processing-addendum.txt  \n",
            "  inflating: ./openai_tos/10_coordinated-vulnerability-disclosure-policy.txt  \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import nest_asyncio\n",
        "import getpass\n",
        "from google.colab import drive\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Turn on async\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Enter OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NAGt-_iUy-r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "1bdc99aa-44b3-4e3e-d06c-4caa8b4b6808"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /usr/local/lib/python3.10/dist-\n",
            "[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /usr/local/lib/python3.10/dist-\n",
            "[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from llama_index.core import (\n",
        "    Document,\n",
        "    VectorStoreIndex,\n",
        "    StorageContext,\n",
        "    load_index_from_storage\n",
        ")\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import PromptTemplate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FeRH8OE6Vff"
      },
      "source": [
        "# Basic RAG system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQPmrii6-TTr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f47754a6-6c68-4eb0-9a7d-19a0481fb3b7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "## Collect the OpenAI TOS docs\n",
        "!unzip ../data/openai_tos.zip ../data/openai_tos\n",
        "directory = '../data/openai_tos/'\n",
        "doc_names = sorted(os.listdir(directory))\n",
        "\n",
        "## Collect each TOS doc into a llama_index Document object\n",
        "documents = []\n",
        "for i, doc_name in enumerate(doc_names):\n",
        "  document = open(directory+doc_name).read()\n",
        "  d = Document(\n",
        "      text=document,\n",
        "      metadata = {\"file\": doc_name, \"name\": doc_name.split('_')[1].split('.')[0].replace('-',' ')}\n",
        "  )\n",
        "  documents.append(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaf9GIOT_V3c"
      },
      "outputs": [],
      "source": [
        "# Define a text chunking procedure\n",
        "text_chunker = SentenceSplitter(chunk_size=128, chunk_overlap=8)\n",
        "# Split the documnets into nodes\n",
        "nodes = text_chunker.get_nodes_from_documents(documents)\n",
        "# Load a model for embedding the text\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqDz0Ex3bf7h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "10661fdc-3bbf-49c1-d894-800338dbb11e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "output_dir = '../data/bge-small-en-v1.5_openai-tos_vectors/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "lkjW2f7bKlSx",
        "outputId": "545d584f-b1f5-48a4-f01d-5385d0d2aaa6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "## Generate indexing vectors\n",
        "\n",
        "if True:\n",
        "    index = VectorStoreIndex(\n",
        "        nodes,\n",
        "        embed_model=embed_model,\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    ## Save embeddings with a storage context.\n",
        "    index.storage_context.persist(persist_dir = output_dir)\n",
        "\n",
        "## Load embeds from storage context. Requires setting the same storage_context as when generated.\n",
        "else:\n",
        "    storage_context = StorageContext.from_defaults(persist_dir = output_dir)\n",
        "    index = load_index_from_storage(\n",
        "        storage_context=storage_context,\n",
        "        embed_model=embed_model\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzCukcbGNFAs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "cf74522c-92d5-472c-b3b4-6af935a83a44"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "## Create a simple query engine and compare to GPT responses\n",
        "\n",
        "def gpt_and_rag_answers(query,query_engine,llm_engine):\n",
        "    llm_response = llm_engine.complete(query)\n",
        "    rag_response = query_engine.query(query)\n",
        "\n",
        "    print(f'############\\nLLM response:\\n{llm_response}\\n\\n############\\nRAG response:\\n{rag_response}')\n",
        "    return llm_response, rag_response\n",
        "\n",
        "## Create a GPT-4 object for API calls\n",
        "llm_gpt = OpenAI(model='gpt-4',temperature=0.0)\n",
        "query_engine = index.as_query_engine(llm=llm_gpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "ChEcCza8bkzV",
        "outputId": "ea271a56-437d-4d64-9955-e015351e6578"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############\n",
            "LLM response:\n",
            "OpenAI retains the ownership of the content created by its programs. However, the user who uses the program to generate content has the rights to that specific output. OpenAI does not claim any ownership over the content generated by users through its programs.\n",
            "\n",
            "############\n",
            "RAG response:\n",
            "The content created by OpenAI programs is owned by the user. OpenAI assigns all its right, title, and interest, if any, in and to the output to the user.\n"
          ]
        }
      ],
      "source": [
        "#query = 'Who owns the content created on OpenAI resources?'\n",
        "query = 'Who owns the content created by OpenAI programs?'\n",
        "gpt, rag = gpt_and_rag_answers(query, query_engine, llm_gpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfCfluEv6arN"
      },
      "source": [
        "# Improved RAG Querying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ZTLNxkXcQ9w3",
        "outputId": "26c6f3be-187e-40ea-c89d-0065f48ca558"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from llama_index.core import get_response_synthesizer, PromptTemplate\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.postprocessor import LLMRerank\n",
        "from llama_index.core.response_synthesizers import ResponseMode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "oVCL07NyRVBy",
        "outputId": "4eb09ff8-990c-4712-93a0-0c337e850df1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "## Create improved querying engine, and querying engine with re-ranking\n",
        "\n",
        "TopK = 5\n",
        "\n",
        "Rerank_TopK = 20\n",
        "Rerank_TopRRK = 5\n",
        "\n",
        "qa_prompt_tmpl = PromptTemplate(\n",
        "    \"You are an expert Q&A system that is trusted around the world.\"\n",
        "    \"Always answer the query using the provided context information, and not prior knowledge.\\n\"\n",
        "    \"Context information is below.\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Given the context information and not prior knowledge, answer the query.\\n\"\n",
        "    \"Query: {query_str}\\n\"\n",
        "    \"Answer: \"\n",
        ")\n",
        "\n",
        "###################\n",
        "\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=TopK,\n",
        ")\n",
        "\n",
        "response_synthesizer = get_response_synthesizer(response_mode=ResponseMode.SIMPLE_SUMMARIZE)\n",
        "\n",
        "custom_query_engine = RetrieverQueryEngine.from_args(\n",
        "    retriever,\n",
        "    response_synthesizer=response_synthesizer,\n",
        ")\n",
        "custom_query_engine.update_prompts(\n",
        "     {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
        ")\n",
        "\n",
        "###################\n",
        "\n",
        "retriever_rr = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=Rerank_TopK,\n",
        ")\n",
        "\n",
        "response_synthesizer_rr = get_response_synthesizer(response_mode=ResponseMode.SIMPLE_SUMMARIZE)\n",
        "\n",
        "custom_query_engine_rerank = RetrieverQueryEngine.from_args(\n",
        "    retriever_rr,\n",
        "    response_synthesizer=response_synthesizer_rr,\n",
        "    node_postprocessors=[\n",
        "        LLMRerank(\n",
        "            choice_batch_size=5,\n",
        "            top_n=Rerank_TopRRK,\n",
        "        )],\n",
        ")\n",
        "custom_query_engine_rerank.update_prompts(\n",
        "     {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "f8I6pbAE-1fg",
        "outputId": "165307f3-c18e-451d-f7bc-b16a8d83806e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############\n",
            "LLM response:\n",
            "OpenAI has not publicly stated that they offer rewards for finding issues with their software. However, they would likely appreciate the feedback to help improve their systems. It's always a good idea to reach out to them directly for the most accurate information.\n",
            "\n",
            "############\n",
            "RAG response:\n",
            "No, OpenAI does not mention providing rewards for finding issues with their software in the provided context information.\n"
          ]
        }
      ],
      "source": [
        "query = 'Will OpenAI give me any sort of reward if I find an issue with their software?'\n",
        "\n",
        "gpt, rag = gpt_and_rag_answers(query, custom_query_engine, llm_gpt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'Will OpenAI give me any sort of reward if I find an issue with their software?'\n",
        "\n",
        "gpt, rag_rr = gpt_and_rag_answers(query, custom_query_engine_rerank, llm_gpt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "20ohNMOdk7iJ",
        "outputId": "a94d5c64-4d14-45b5-8232-92e6e92e6f10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############\n",
            "LLM response:\n",
            "OpenAI has not publicly announced any bug bounty programs or rewards for finding issues in their software. However, they would likely appreciate being informed about any potential issues or bugs. You can contact them through their official website or other official communication channels. Always remember to follow responsible disclosure guidelines when reporting potential issues.\n",
            "\n",
            "############\n",
            "RAG response:\n",
            "Yes, OpenAI offers a Bug Bounty Program as a way to recognize and reward security researchers who contribute to keeping their technology and company secure.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning"
      ],
      "metadata": {
        "id": "PeUzJFn-Ox2b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "XGSP6kmVrTot",
        "outputId": "3bfd7738-302d-4ab8-f49f-8f1d7bbdafcc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from llama_index.finetuning import generate_qa_embedding_pairs\n",
        "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "R-txihlMzCkC",
        "outputId": "7eadd123-355c-4833-f7d4-6271f0a4c3d0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "260 66\n"
          ]
        }
      ],
      "source": [
        "train_nodes, val_nodes = train_test_split(pd.Series(nodes), test_size=0.20, random_state=7)\n",
        "print(len(train_nodes), len(val_nodes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "COa2uHEsZmN-",
        "outputId": "2692faa9-4b1a-4167-8fc4-a5726f15defd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "json_locs = '../data/bge-small-en-v1.5_openai-tos_qa-embedding-pairs/'\n",
        "!mkdir $json_locs\n",
        "\n",
        "if True:\n",
        "    train_dataset = generate_qa_embedding_pairs(\n",
        "        train_nodes,\n",
        "        llm=OpenAI(model='gpt-3.5-turbo',temperature=0.0),\n",
        "    )\n",
        "    val_dataset = generate_qa_embedding_pairs(\n",
        "        val_nodes,\n",
        "        llm=OpenAI(model='gpt-3.5-turbo',temperature=0.0),\n",
        "    )\n",
        "\n",
        "    train_dataset.save_json(json_locs+\"train_dataset.json\")\n",
        "    val_dataset.save_json(json_locs+\"val_dataset.json\")\n",
        "\n",
        "else:\n",
        "    train_dataset = EmbeddingQAFinetuneDataset.from_json(json_locs+\"train_dataset.json\")\n",
        "    val_dataset = EmbeddingQAFinetuneDataset.from_json(json_locs+\"val_dataset.json\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Look at validation set queries and find an interesting example\n",
        "val_dataset.__dict__['queries']"
      ],
      "metadata": {
        "id": "satHQ8LxmBtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "3TnvEzIFlkq5",
        "outputId": "0bf1cb45-a33c-4d69-dedf-20569c33b91a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample text chunk:\n",
            "------------------\n",
            "If you notice that ChatGPT output contains factually inaccurate information about you and you would like us to correct the inaccuracy, you may submit a correction request through privacy.openai.com or to dsar@openai.com. Given the technical complexity of how our models work, we may not be able to correct the inaccuracy in every instance. In that case, you may request that we remove your Personal Information from ChatGPT’s output by filling out this form.\n",
            "\n",
            "\n",
            "Query based on chunk:\n",
            "---------------------\n",
            "How can individuals request corrections for factually inaccurate information about themselves in ChatGPT output?\n"
          ]
        }
      ],
      "source": [
        "query_id = '0222418f-4e63-4577-84ec-94597552bff4'\n",
        "corpus_id = val_dataset.__dict__['relevant_docs'][query_id][0]\n",
        "\n",
        "sample_chunk = val_dataset.__dict__['corpus'][corpus_id]\n",
        "sample_query = val_dataset.__dict__['queries'][query_id]\n",
        "\n",
        "print(f'Sample text chunk:\\n------------------\\n{sample_chunk}')\n",
        "print(f'\\n\\nQuery based on chunk:\\n---------------------\\n{sample_query}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "RM3Kryuau68c",
        "outputId": "aadefae8-3f61-4577-a42b-a045ad466935"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from llama_index.finetuning import SentenceTransformersFinetuneEngine\n",
        "\n",
        "model_ft_path = '../data/bge-small-en-v1.5_openai-tos_finetuned-model'\n",
        "\n",
        "if False:\n",
        "    finetune_engine = SentenceTransformersFinetuneEngine(\n",
        "        model_id = \"BAAI/bge-small-en-v1.5\",\n",
        "        dataset = train_dataset,\n",
        "        val_dataset = val_dataset,\n",
        "        epochs = 4,\n",
        "        model_output_path = model_ft_path\n",
        "    )\n",
        "    finetune_engine.finetune()\n",
        "    finetuned_embedding_model = finetune_engine.get_finetuned_model()\n",
        "\n",
        "else:\n",
        "    finetuned_embedding_model = HuggingFaceEmbedding(\n",
        "        model_name = model_ft_path\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "HZvHiSr-_Zw1",
        "outputId": "14278e4d-2bef-472c-80aa-1020f04caff6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pathlib import Path\n",
        "\n",
        "def evaluate_st(dataset, model_id, name):\n",
        "    corpus = dataset.corpus\n",
        "    queries = dataset.queries\n",
        "    relevant_docs = dataset.relevant_docs\n",
        "\n",
        "    evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs, name=name)\n",
        "    model = SentenceTransformer(model_id)\n",
        "    output_path = \"results/\"\n",
        "    Path(output_path).mkdir(exist_ok=True, parents=True)\n",
        "    return evaluator(model, output_path=output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "w6KM4i5M_Zux",
        "outputId": "d0564e49-630c-42a0-a3d6-fe6d056bdc3c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9361111111111111"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "evaluate_st(val_dataset, \"BAAI/bge-small-en-v1.5\", name=\"bge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "2QuHEP9z_Zsl",
        "outputId": "894f249c-9f94-4f41-a18d-e3c7e0526b1a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9551767676767677"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "evaluate_st(val_dataset, model_ft_path, name=\"finetuned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "de08e9b6922a4934815e63354bfe0b0a",
            "dd6e6215460849959c92866cc17ce85f",
            "dacd34e84a2e4754872210ec901c1ec8",
            "b4f88bbd8d424c3ba56695acfa6f1325",
            "e92d69c4d9fb4d788f5d0df9502a8a3f",
            "1ff8d58a5e0144a881490556342e1093",
            "7c91f8d8572f4a22a673ba5726dc5366",
            "d820020f1d3541d683aa812bb058dd5a",
            "199d062a76584d3d8d68ae0ac9e9485b",
            "5da5865a40bb49ffac036b4588c244f5",
            "50bdf5871b57454d8505b969e5441759"
          ]
        },
        "id": "PsiKkwOfeU6t",
        "outputId": "6c2c8a62-9a09-461b-841d-e03395f24f61"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/326 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de08e9b6922a4934815e63354bfe0b0a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "output_dir = '../data/bge-small-en-v1.5_openai-tos_vectors'\n",
        "\n",
        "if True:\n",
        "    ## Generate new vector-index with the fine-tuned embedding model\n",
        "    index_ft = VectorStoreIndex(\n",
        "        nodes,\n",
        "        embed_model = finetuned_embedding_model,\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    ## Save embeddings with a storage context.\n",
        "    index_ft.storage_context.persist(persist_dir = output_dir)\n",
        "\n",
        "else: ## Load vectors\n",
        "    storage_context = StorageContext.from_defaults(persist_dir = output_dir)\n",
        "    index_ft = load_index_from_storage(\n",
        "        storage_context=storage_context,\n",
        "        embed_model=finetuned_embedding_model\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "SH9BoRLov6Du",
        "outputId": "3c0b17d8-f516-484d-db0b-542debd2f11a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "ft_query_engine = index_ft.as_query_engine()\n",
        "\n",
        "def ft_gpt_and_rag_answers(query,query_engine,llm_engine):\n",
        "    #gpt_response = OpenAI(model=gpt_model).complete(query)\n",
        "    llm_response = llm_engine.complete(query)\n",
        "    rag_response = query_engine.query(query)\n",
        "\n",
        "    print(f'############\\nLLM response:\\n{llm_response}\\n\\n############\\nRAG response:\\n{rag_response}')\n",
        "    return llm_response, rag_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "OpMhCRcpsUOE",
        "outputId": "dd032a28-9f65-491b-946a-bfa91cdd2e90"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "## Create\n",
        "\n",
        "TopK = 5\n",
        "\n",
        "Rerank_TopK = 20\n",
        "Rerank_TopRRK = 5\n",
        "\n",
        "qa_prompt_tmpl = PromptTemplate(\n",
        "    \"You are an expert Q&A system that is trusted around the world.\"\n",
        "    \"Always answer the query using the provided context information, and not prior knowledge.\\n\"\n",
        "    \"Context information is below.\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Given the context information and not prior knowledge, answer the query.\\n\"\n",
        "    \"Query: {query_str}\\n\"\n",
        "    \"Answer: \"\n",
        ")\n",
        "\n",
        "###################\n",
        "\n",
        "retriever_ft = VectorIndexRetriever(\n",
        "    index=index_ft,\n",
        "    similarity_top_k=TopK,\n",
        ")\n",
        "\n",
        "response_synthesizer_ft = get_response_synthesizer(response_mode=ResponseMode.SIMPLE_SUMMARIZE)\n",
        "\n",
        "custom_query_engine_ft = RetrieverQueryEngine.from_args(\n",
        "    retriever_ft,\n",
        "    response_synthesizer=response_synthesizer_ft,\n",
        ")\n",
        "custom_query_engine_ft.update_prompts(\n",
        "     {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
        ")\n",
        "\n",
        "###################\n",
        "\n",
        "retriever_rr_ft = VectorIndexRetriever(\n",
        "    index=index_ft,\n",
        "    similarity_top_k=Rerank_TopK,\n",
        ")\n",
        "\n",
        "response_synthesizer_rr_ft = get_response_synthesizer(response_mode=ResponseMode.SIMPLE_SUMMARIZE)\n",
        "\n",
        "custom_query_engine_rerank_ft = RetrieverQueryEngine.from_args(\n",
        "    retriever_rr_ft,\n",
        "    response_synthesizer=response_synthesizer_rr_ft,\n",
        "    node_postprocessors=[\n",
        "        LLMRerank(\n",
        "            choice_batch_size=5,\n",
        "            top_n=Rerank_TopRRK,\n",
        "        )],\n",
        ")\n",
        "custom_query_engine_rerank_ft.update_prompts(\n",
        "     {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'How does ChatGPT deal with inappropriate questions?'\n",
        "\n",
        "custom_query_engine.query(query).response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "1iA1CfKDnShv",
        "outputId": "987b2ff9-f2d3-42ae-8344-208929b8e0b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ChatGPT ensures that automated systems, such as chatbots, disclose to people that they are interacting with AI unless it is obvious from the context. Additionally, ChatGPT does not build tools that may be inappropriate for minors, including sexually explicit or suggestive content, unless it is created for scientific or educational purposes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "9r6d3i2n0nEv",
        "outputId": "fe6ddb62-a94e-42fb-973d-6c183c03722f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ChatGPT deals with inappropriate questions by not allowing the creation of tools that may be inappropriate for minors, such as sexually explicit or suggestive content. Additionally, ChatGPT prohibits generating or promoting disinformation, misinformation, or false online engagement, impersonating another individual or organization without consent, engaging in or promoting academic dishonesty, using content from third parties without necessary permissions, and misrepresenting or misleading others about the purpose of the GPT.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 126
        }
      ],
      "source": [
        "custom_query_engine_ft.query(query).response"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "7aS-b6kZPFEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context Relevance -- Is the retrieved context relevant to the query?"
      ],
      "metadata": {
        "id": "x4EZ9xtxv2PO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.evaluation import RetrieverEvaluator\n",
        "\n",
        "def run_context_relevance_eval(index, queries, expected_ids, rerank=False):\n",
        "    if rerank:\n",
        "        retriever = VectorIndexRetriever(\n",
        "            index=index,\n",
        "            similarity_top_k=20,\n",
        "            node_postprocessors=[\n",
        "                LLMRerank(\n",
        "                    choice_batch_size=5,\n",
        "                    top_n=2,\n",
        "                )],\n",
        "        )\n",
        "    else:\n",
        "        retriever = index.as_retriever(similarity_top_k=2)\n",
        "\n",
        "    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
        "        [\"mrr\", \"hit_rate\"], retriever=retriever\n",
        "    )\n",
        "\n",
        "    context_eval_results = []\n",
        "    for cid, q in zip(expected_ids,queries):\n",
        "        context_relev_eval = retriever_evaluator.evaluate(\n",
        "            query=q, expected_ids=[cid,]\n",
        "        )\n",
        "        context_eval_results.append(context_relev_eval)\n",
        "\n",
        "    eval_df = pd.DataFrame({\n",
        "        'query': [cer.query for cer in context_eval_results],\n",
        "        'expected_ids': [cer.expected_ids for cer in context_eval_results],\n",
        "        'retrieved_ids': [cer.retrieved_ids for cer in context_eval_results],\n",
        "        'mrr': [cer.metric_dict['mrr'].score for cer in context_eval_results],\n",
        "        'hit_rate': [cer.metric_dict['hit_rate'].score for cer in context_eval_results]\n",
        "    })\n",
        "\n",
        "    print('Total MRR = ',eval_df.mrr.sum(),'/ ',len(queries))\n",
        "    print('# Hits = ',eval_df.hit_rate.sum(),'/ ',len(queries))\n",
        "    return eval_df\n",
        "\n",
        "## First lets test on our sample Q&A pair\n",
        "query_id = '0222418f-4e63-4577-84ec-94597552bff4'\n",
        "corpus_id = val_dataset.__dict__['relevant_docs'][query_id][0]\n",
        "sample_chunk = val_dataset.__dict__['corpus'][corpus_id]\n",
        "sample_query = val_dataset.__dict__['queries'][query_id]\n",
        "\n",
        "print('Query:\\n', sample_query)\n",
        "results = run_context_relevance_eval(index,[sample_query,],[corpus_id,])\n",
        "\n",
        "print('Expected ID = ',results.iloc[0].expected_ids)\n",
        "print('Retrieved IDs = ',results.iloc[0].retrieved_ids)\n",
        "print('Top doc:\\n',val_dataset.__dict__['corpus'][results.iloc[0].retrieved_ids[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "PkcfWbbhaHYc",
        "outputId": "4b417e24-a7ec-417e-b21e-5110ff074cd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n",
            " How can individuals request corrections for factually inaccurate information about themselves in ChatGPT output?\n",
            "Total MRR =  1.0 /  1\n",
            "# Hits =  1.0 /  1\n",
            "Expected ID =  ['f712129a-a58d-4e36-b62f-22ebfeda56a8']\n",
            "Retrieved IDs =  ['f712129a-a58d-4e36-b62f-22ebfeda56a8', '1f95b363-1002-4f2d-bf17-ab4842714072']\n",
            "Top doc:\n",
            " If you notice that ChatGPT output contains factually inaccurate information about you and you would like us to correct the inaccuracy, you may submit a correction request through privacy.openai.com or to dsar@openai.com. Given the technical complexity of how our models work, we may not be able to correct the inaccuracy in every instance. In that case, you may request that we remove your Personal Information from ChatGPT’s output by filling out this form.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Convert the val_dataset Q&A results to a dictionary linking each query\n",
        "## to the document it was built from\n",
        "query_ids = list(val_dataset.__dict__['queries'].keys())\n",
        "corpus_ids = [val_dataset.__dict__['relevant_docs'][qid][0] for qid in query_ids]\n",
        "queries = [val_dataset.__dict__['queries'][qid] for qid in query_ids]\n",
        "expected_texts = [val_dataset.__dict__['corpus'][cid] for cid in corpus_ids]\n",
        "## Create a label mapping from non-finetuned document indexing to fine-tuned doc\n",
        "## indexing, so we know which document to expect when running the FT model.\n",
        "index_ids = list(index.__dict__['_index_struct'].nodes_dict.keys())\n",
        "index_ft_ids = list(index_ft.__dict__['_index_struct'].nodes_dict.keys())\n",
        "ft_corpus_ids = {index_ids[i]: index_ft_ids[i] for i in range(len(index_ids))}\n",
        "\n",
        "keys_df = pd.DataFrame({\n",
        "    'query_ids': query_ids,\n",
        "    'corpus_ids': corpus_ids,\n",
        "    'ft_corpus_ids': [ft_corpus_ids[cid] for cid in corpus_ids],\n",
        "    'queries': queries,\n",
        "    'expected_texts': expected_texts\n",
        "}).sample(50,random_state=7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "M11mayeVw3Ii",
        "outputId": "5e4c59d7-a04e-4c4c-ec69-91c5c553062b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = run_context_relevance_eval(index,keys_df['queries'],keys_df['corpus_ids'],rerank=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "8bVumAZ_vTnS",
        "outputId": "b055700d-c891-4e20-e0cb-dd51df8a242d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total MRR =  37.526923076923076 /  50\n",
            "# Hits =  49.0 /  50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_ft = run_context_relevance_eval(index_ft,keys_df['queries'],keys_df['ft_corpus_ids'],rerank=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "1LcnAje_rwD6",
        "outputId": "bf5c22fd-a8fe-42f9-d29d-843e8c6f764b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total MRR =  40.7 /  50\n",
            "# Hits =  49.0 /  50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer Relevance -- Is the generated answer relevant to the query?"
      ],
      "metadata": {
        "id": "0aFFFnXSv7WX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "BOF9qVOW1dj-",
        "outputId": "3a5ae67e-f194-46d1-dfe5-829e197874e4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:04<00:00,  4.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Individuals can request corrections for factually inaccurate information about themselves in ChatGPT output by submitting a correction request through privacy.openai.com or by sending an email to dsar@openai.com. If the inaccuracy cannot be corrected due to the technical complexity of the models, they can request the removal of their Personal Information from ChatGPT’s output by filling out a specific form.\n",
            "True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "## TruLens: Answer Relevance -- Is the answer relevant to the query?\n",
        "\n",
        "from llama_index.core.evaluation import RelevancyEvaluator\n",
        "\n",
        "def run_answer_relevance_eval(index,queries,rerank=False):\n",
        "\n",
        "    if rerank:\n",
        "        query_engine = index.as_query_engine(\n",
        "            llm=llm_gpt,\n",
        "            node_postprocessors=[\n",
        "                LLMRerank(choice_batch_size=5, top_n=2,\n",
        "            )],\n",
        "        )\n",
        "    else:\n",
        "        query_engine = index.as_query_engine(llm=llm_gpt)\n",
        "    ans_relev_evaluator = RelevancyEvaluator(llm=llm_gpt)\n",
        "\n",
        "    answer_eval_results = []\n",
        "    for query in tqdm(queries):\n",
        "        response = query_engine.query(query)\n",
        "        ans_relev_eval = ans_relev_evaluator.evaluate_response(query=query, response=response)\n",
        "        answer_eval_results.append(ans_relev_eval)\n",
        "\n",
        "    print(sum([aer.passing for aer in answer_eval_results]))\n",
        "    return answer_eval_results\n",
        "\n",
        "query = \"How can individuals request corrections for factually inaccurate information about themselves in ChatGPT output?\"\n",
        "results = run_answer_relevance_eval(index,[query,],rerank=True)\n",
        "\n",
        "print(results[0].response)\n",
        "print(str(results[0].passing))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = run_answer_relevance_eval(index,keys_df['queries'],rerank=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "K3ghqFSP001z",
        "outputId": "61a1efdb-1b80-4668-928b-b6caa2985537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [03:24<00:00,  4.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_ft = run_answer_relevance_eval(index_ft,keys_df['queries'],rerank=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "I46RH0hq0-Qb",
        "outputId": "bfe2d89e-8516-41c3-a36f-11e74a89a7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [03:29<00:00,  4.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Groundedness -- Is the response supported by the context?"
      ],
      "metadata": {
        "id": "RuPC4jAZJhSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
        "\n",
        "def run_groundedness_eval(index,queries,rerank=False):\n",
        "\n",
        "    if rerank:\n",
        "        query_engine = index.as_query_engine(\n",
        "            llm=llm_gpt,\n",
        "            node_postprocessors=[\n",
        "                LLMRerank(choice_batch_size=5, top_n=2,\n",
        "            )],\n",
        "        )\n",
        "    else:\n",
        "        query_engine = index.as_query_engine(llm=llm_gpt)\n",
        "    grnd_eval = FaithfulnessEvaluator(llm=llm_gpt)\n",
        "\n",
        "    # responses = [query_engine.query(q) for q in queries]\n",
        "    # grnd_eval_results = [grnd_eval.evaluate_response(r) for r in responses]\n",
        "    ground_eval_results = []\n",
        "    for query in tqdm(queries):\n",
        "        response = query_engine.query(query)\n",
        "        ground_eval = faithfulness_eval.evaluate_response(response=response)\n",
        "        ground_eval_results.append(ground_eval)\n",
        "\n",
        "    print(sum([ger.passing for ger in ground_eval_results]))\n",
        "    return ground_eval_results\n",
        "\n",
        "query = \"How can individuals request corrections for factually inaccurate information about themselves in ChatGPT output?\"\n",
        "results = run_answer_relevance_eval(index,[query,],rerank=True)\n",
        "\n",
        "print('Contexts: ',results[0].contexts)\n",
        "print('\\nResponse: ',results[0].response)\n",
        "print('\\n',str(results[0].passing))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "GrNKXPEXJzux",
        "outputId": "2f91e96c-4fd1-47ff-e86e-b8779dd47c4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:03<00:00,  3.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Contexts:  ['If you notice that ChatGPT output contains factually inaccurate information about you and you would like us to correct the inaccuracy, you may submit a correction request through privacy.openai.com or to dsar@openai.com. Given the technical complexity of how our models work, we may not be able to correct the inaccuracy in every instance. In that case, you may request that we remove your Personal Information from ChatGPT’s output by filling out this form.', 'If you are unable to exercise your rights through your account, please submit your request through privacy.openai.com or to dsar@openai.com.\\n\\nA note about accuracy: Services like ChatGPT generate responses by reading a user’s request and, in response, predicting the words most likely to appear next. In some cases, the words most likely to appear next may not be the most factually accurate. For this reason, you should not rely on the factual accuracy of output from our models.']\n",
            "\n",
            "Response:  Individuals can request corrections for factually inaccurate information about themselves in ChatGPT output by submitting a correction request through privacy.openai.com or by sending an email to dsar@openai.com. If the inaccuracy cannot be corrected due to the technical complexity of the models, they can request the removal of their Personal Information from ChatGPT’s output by filling out a specific form.\n",
            "\n",
            " True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = run_groundedness_eval(index,keys_df['queries'],rerank=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "d1n4nWnVJXln",
        "outputId": "a9ee6f26-4ee2-4e9a-ea10-ea1b133897e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [03:28<00:00,  4.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_ft = run_groundedness_eval(index_ft,keys_df['queries'],rerank=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "9indLTiFaHBU",
        "outputId": "92d6064a-30ca-499a-c309-70e31cd02d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [03:29<00:00,  4.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create dataframes of the output to example results\n",
        "\n",
        "grnd_df = pd.DataFrame({\n",
        "    'query': keys_df['queries'],\n",
        "    'contexts': [r.contexts for r in results],\n",
        "    'response': [r.response for r in results],\n",
        "    'pass': [str(r.passing) for r in results],\n",
        "    'score': [r.score for r in results]\n",
        "})\n",
        "\n",
        "grnd_df_ft = pd.DataFrame({\n",
        "    'query': keys_df['queries'],\n",
        "    'contexts': [r.contexts for r in results_ft],\n",
        "    'response': [r.response for r in results_ft],\n",
        "    'pass': [str(r.passing) for r in results_ft],\n",
        "    'score': [r.score for r in results_ft]\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "6XicR8VBMLK3",
        "outputId": "27230798-2ece-4334-d7a3-d800e4314c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "de08e9b6922a4934815e63354bfe0b0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd6e6215460849959c92866cc17ce85f",
              "IPY_MODEL_dacd34e84a2e4754872210ec901c1ec8",
              "IPY_MODEL_b4f88bbd8d424c3ba56695acfa6f1325"
            ],
            "layout": "IPY_MODEL_e92d69c4d9fb4d788f5d0df9502a8a3f"
          }
        },
        "dd6e6215460849959c92866cc17ce85f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ff8d58a5e0144a881490556342e1093",
            "placeholder": "​",
            "style": "IPY_MODEL_7c91f8d8572f4a22a673ba5726dc5366",
            "value": "Generating embeddings: 100%"
          }
        },
        "dacd34e84a2e4754872210ec901c1ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d820020f1d3541d683aa812bb058dd5a",
            "max": 326,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_199d062a76584d3d8d68ae0ac9e9485b",
            "value": 326
          }
        },
        "b4f88bbd8d424c3ba56695acfa6f1325": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5da5865a40bb49ffac036b4588c244f5",
            "placeholder": "​",
            "style": "IPY_MODEL_50bdf5871b57454d8505b969e5441759",
            "value": " 326/326 [00:01&lt;00:00, 325.02it/s]"
          }
        },
        "e92d69c4d9fb4d788f5d0df9502a8a3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ff8d58a5e0144a881490556342e1093": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c91f8d8572f4a22a673ba5726dc5366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d820020f1d3541d683aa812bb058dd5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "199d062a76584d3d8d68ae0ac9e9485b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5da5865a40bb49ffac036b4588c244f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50bdf5871b57454d8505b969e5441759": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}